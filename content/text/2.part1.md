<br class="breakpage">

# Browsing

Definition Browsing:

* an act of casual looking or reading.
* (of an animal) feed on leaves, twigs, or other high-growing vegetation.


## Information Before the World Wide Web 

(figure: content/images/commonplace-book.jpg caption: Commonplace book, mid-17th century)

Before the World Wide Web was born, information was compiled and stored physically in the hands of individuals. During the Renaissance period, individuals started their own knowledge management for example with the idea of a commonplace book. This written journal or "database" serves as a personal compilation of one's individual pool of knowledge, ideas, quotations, and observations, gathered for future reference or reflection. It differs from a conventional journal or log, by indexing by subject instead of a chronological classification of information. In the same period, cabinet of curiosities emerged as entertainment spaces for learning – either rooms or pieces of furnitures – gathered by privileged curators, especially members of aristocracy, merchants or early science practitioners. Those collections driven either by aesthetic concerns or more practical and scholar endeavors, were not only aiming to tend the curiosity of its owner but was also acting as a representation of social status to uphold rank in society or in the scientific practice. From such collections often, catalogs were printed and shared in clubs to help discuss how knowledge can be categorized.

Later on, a more scholarly and scientific information management emerged such as _Zettelkasten_, german translation for “card files”. This significantly changed how information can be linked by shifting linear bound commonplace books and catalogs to indexed cards stored into card files and later on, file cabinets. The idea behind such a method of organisation was for individual notes to be rearranged at any time, either being grouped by subject headings, tags or other metadata to interlink notes. This information system is a key precursor to future hypertext personal knowledge bases, such as NoteCards or later on, collaborative wikis within the World Wide Web era.

In the early twentieth century, this file cabinets approach to storing information was reused at much larger scale by belgian Bibliographer and peace activist Paul Otlet. He founded the Mundaneum in 1919 after the first World War, in Belgium, an institution aiming to gather all the world's knowledge and to classify it according to a system called the Universal Decimal Classification. From initially storing 400000 entries to more than 15 million index cards in 1934 when it closed, this project – regarded by Otlet as “World City” to radiate knowledge to the rest of the world and construct peace and universal cooperation –  is considered an analog ancestor of the Internet, search engines and especially of systematic knowledge networks such as Wikipedia developed later in the same century. Otlet’s *Traité de documentation: le livre sur le livre, théorie et pratique* written in 1934, even prefigured concepts of contemporary computer interfaces such as the multiple desktops and tab systems, as well as the reading and annotation of remote documents into collective databases and computer speech.

(figure: content/images/mundaneum.jpg caption: Mundaneum, founded by Paul Otlet, 1919–34, Belgium)

One key proposition in his essay, “the radiated library and the televised book,”[!otlet] was a novel scheme for remote access to data with minimal use of hard copy. Otlet proposed a global network of “electric telescopes.” These early workstations were to be linked to the Mundaneum by telephone and television, new technology at the time. By phoning in a query, the user would get their answer from a book or other source displayed onto a personal screen, which could be split to show multiple results. The network would support audio output, too, and in a final, startlingly predictive touch, Otlet’s system would also enable data sharing and social interactions among its users.

Unfortunately, Otlet’s propositions were never realised, and his Mundaneum, pointing universality yet remaining eurocentric, came to an end in 1934 with cuts in funding due to the second World War. Moreover, it is important to emphasize that while his vision to promote the spreading of information was ground-breaking for Information Sciences, his problematic, personal, white male eurocentric views on the superiority of European culture and intelligence, lead to many racist statements. Claims such as the biological superiority of white people, or that white people or 'westernized' blacks were to be tasked with “civilising” Africa, reveal questions on underlying aspects from his centralised archive, considering the broader context in which European civilizing mission to expanded itself through colonization. Especially, questioning the archive on its aspects of universality biased by western-centric, and neutrality, biaised by the dominance and exclusion of a plurality of sources and point of views. 

[!otlet]: Levie, Françoise, 2002, _The Man Who Wanted to Classify the World_, [online], Memento/Sofidoc Production, [https://www.youtube.com/watch?v=HieMJSgnkSE](https://www.youtube.com/watch?v=HieMJSgnkSE) [consulted: 30 October 2023]


## Rise of the Information Age

At the end of second World War, a shift in traditional industries established during the industrial revolutions emerged, positioning in the foreground a new economic model in which advances in information technology could play a key role. Members of the scientific community started seeking ways of maintaining the peace through understanding, collaboration and sharing of their research to a broader network, instead of current scientific efforts directed toward destruction.

In july 1945, the American engineer Vannevar Bush published “As We May Think”, a highly influential article in the Atlantic Monthly, right before the bombing of Hiroshima and Nagasaki. Bush expresses in his article a desire for a sort of collective memory as a mean for peace and collaboration in the scientific community, to be accessible through future devices, such as his hypothetical object, the Memex, to interact with global pools of knowledge, interconnecting disparate informations. His writing are considered to predict many of the current information technologies still used today, including Hypertext, Personal computers, Internet and the World Wide Web, Speech recognition and online encyclopedias such as Wikipedia.

> "Wholly new forms of encyclopedias will appear, ready-made with a mesh of associative trails running through them, ready to be dropped into the memex and there amplified."[!Bush]

[!Bush]: BUSH, Vannevar, 1945. As We May Think. The Atlantic. 1945.  

(figure: content/images/memex.jpg caption: Illustration of the Memex, a fictional device mentioned in _As We May Think_, Vannevar Bush, The Atlantic Monthly, 1945.)

Following up on Bush’s desires of sharing and accessing knowledge globally, advances in hardware miniaturization considerably helped the modernization of information systems, with notably the invention of the transistor in 1947. This electric component, key to store information in a computer, helped to gradually shift the management of information, from analog to digital storage. Indeed, hardware improvements led analog storage such as microfilm to be replaced by digital imaging, storage, and transmission media. In order to understand this shift, concepts were formulated such as the Library expansion formulated by Fremont Rider (1945), in which capacity of storage would double every 16 years where sufficient space made available. Later on, Moore’s Law (1965), stated the transistors capacity in integrated circuits would double every two years. In the mid 2000s, an additional law emerged, Kryder’s Law, on how digital storage capacity would evolve similarly exponentially. How then could we deal with the accessibility and navigation of ever-increasing information, resistant to nuclear threats against the backdrop of the Cold War?

In order to connect to this ever-growing digital storage of information, defense projects developped by DARPA, aimed to create decentralized networks of information. Arpanet (1969), is the concrete ancestor of the Internet, and was originally designed explicitly as a decentralized system: the challenge was to build an invulnerable network of computers, i.e. one that could neither be controlled (by an enemy government) nor destroyed (by a nuclear attack). The only solution to such a problem was decentralization: nobody owns the Internet, nobody controls the Internet, nobody decides who can connect to it or what they can do once connected. As Alexander Galloway articulates:[^galloway]

[^galloway]: GALLOWAY, Alexander R., 2006. _Protocol: how control exists after decentralization_. 1. MIT Press paperback ed. Cambridge, Mass. : MIT Press. Leonardo. ISBN 978-0-262-57233-0. 

> While many have debated the origins of the Internet, it’s clear that in many ways it was built to withstand nuclear attack. The Net was designed as a solution to the vulnerability of the military’s centralized system of command and control during the late 1950’s and beyond. For, the argument goes, if there are no central command centers, then there can be no central targets and overall damage is reduced.

For information to circulate, advances in the field of Human Computer Interaction and especially protocols of linking documents to each other were developed. Among those, _Hypertext_ developed by Ted Nelson in 1965, achieved the possibility to link documents non-linearily, creating a network of information via a hyperlink system. While Nelson was conducting from 1960 the first Hypertext project, _Project Xanadu_ (1960), other scientists made advances in Human Computer Interfaces such as Douglas Engelbart, notable for creating the first Graphic User Interfaces among others pillars. In the eighties, Hypertext will be further adopted to develop softwares such as _Hypercard_ – a software application and development kit – by Bill Atkinson in 1987 for Apple, _NoteCards_ – a hypertext-based personal knowledge base – by Randall Trigg, Frank Halasz and Thomas Moran in 1984 for Xerox Park, or the _World Wide Web_ by Tim Berners-Lee and Robert Caillau in 1989 at CERN, Geneva. 

(imagenote: content/images/xanadu.jpg caption: Project Xanadu, 1965, Ted Nelson)

(imagenote: content/images/NoteCards-XeroxPark.png caption: NoteCards, created by Bill Atkinson in 1987 for Apple)

In parallel to developing hardwares and softwares, topics of access to information and tools were conducted on a more public scope with examples such as the printed american periodic _Whole Earth Catalog_,(imagenote: content/images/whole-earth-catalog.jpg caption: First issue of the Whole Earth Catalog, Fall 1969, by Stewart Brand) created by Stewart Brand in 1968, and _Community Memory_ – the first public computerized bulletin board system, established in 1973 in Berkeley, California. (imagenote: content/images/community-memory.jpg caption: Community Memory group photograph with terminal, 1984, Lee Felsenstein Collection, Digital Archive, Catalog 102702620. From left to right, Carl Farrington, Michael Rossman, Phil Kohn, Lee Felsenstein, Karen Paulsell, unknown woman, Ken Constad.)

The Whole Earth Catalog, subtitled “Access to Tools”, was an american counterculture magazine featuring article, essays, and especially product reviews, focusing on ecology, self-sufficiency, DIY, holism and alternative education. Using a NASA satellite photo of the Planet Earth as cover material, Brand encouraging curiosity and alternative access to information as a shared destiny. A farewell message printed on the last 1974 edition of the Catalog stipulated: “Stay hungry, Stay Foolish.” Quoted by Steve Job’s in his June 2005 Stanford University commencement speech[^jobs], Whole Earth Catalog is for him to be considered as an ancestor of the Web search engines technologies:

> When I was young, there was an amazing publication called The Whole Earth Catalog, which was one of the bibles of my generation… It was sort of like Google in paperback form, 35 years before Google came along. It was idealistic and overflowing with neat tools and great notions.

[^jobs]: JOBS, Steve, 2005, Steve Jobs' 2005 Stanford Commencement Address, [online], Stanford, https://www.youtube.com/watch?v=UF8uR6Z6KLc [consulted: 1 November 2023]

Community Memory was among the first experiment in the 1970s of a source of community information and resource sharing network, linking a variety of counter-cultural economic, educational, and social organizations with each other and the public. Using the context of site-specific terminals, for users to access, enter and retrieve informations, it was in the first place an experiment to see how people would react to using a computer to exchange information, in a context when computers were not that publicly accessible at the time. (imagenote: content/images/Berkeley_was_building_online_communities_in_1973_in_local_record_shops._No_wonder_I_love_that_city._(32887108980).jpg caption: Terminal exhibited at the Computer History MuseumBerkeley_was_building_online_communities_in_1973_in_local_record_shops._No_wonder_I_love_that_city.)

In the same years, Ted Nelson published Computer Lib / Dream Machines in 1974, a seminal book about personal computers and the urgent need to get familiar with, stating as a tag line on the front cover[^computerlib]: (imagenote: content/images/computer-lib.webp caption: Cover of Computer Lib / Dream Machines, 1974.)

> You can and must understand computers NOW.

[^computerlib]: NELSON, Theodor H., 1983. Computer Lib. 9. print. South Bend, Ind : The Distributors. ISBN 978-0-89347-002-9. 

When Arpanet and its cross-continent counterparts (such as Cyclades in France, India’s ERNET, South Africa’s UNINET, etc.), merged to form Internet, a handful of networked university computers composed Internet as a global transcontinental networks. Along with hardware evolution such as personal computers, which were slowly sliding in people’s homes, a new technology timely emerged in the late 1980s, the _World Wide Web_.


## From Read-Only to Read-Write: evolution of the World Wide Web

(figure: content/images/timbernerslee.jpg caption: Sir Tim Berners-Lee invented the World Wide Web in 1989 while working at the CERN. (Image: CERN))

The _World Wide Web_, created in 1989 by Tim Berners-Lee at CERN, now commonly called _Web_ was released to the public in 1991, with the underlying motivations to pool resources and help the storing, updating, and finding of information on a global scale. As stated on the CERN website, the Web was originally conceived and developed to meet the demand for automated information-sharing between scientists in universities and institutes around the world. [^BernersLee1] Using Ted Nelson's Hypertext model, navigating in nonlinear ways through disparate documents with hyperlinks has tremendously helped to build a global collection of knowledge hosted on websites and databases, all accessible on interconnected computers. This key technology of the Information Age, using global networks of computers has grown from a few thousand websites in the early nineties to over 1.5 billion websites today. _Browsing_ the Web – meaning to jump from one webpage to another using hyperlinks – has drawn different ways of interacting with information online: one of them being exploratory surfing. In the nineties, before the Web evolved to its social aspects nowadays, it was initially a more *read-only* approach. Users would then essentially *surf* the web through the GUI of web browsers, in a passive manner to explore pages without really editing them or appending content. To facilitate access to online information, the rise of search engines such as Google and Yahoo helped users in their search, while browsers would develop features for more human curatorial actions such as bookmarking webpages. 

> When the internet started, it was pretty natural just to use the internet too. But of course, the books were up there. The transition was pretty gradual, I think it took more like 10 years. At some point I started to realize, I haven't bought a book in like 2 years. That is really weird. And I mostly used just the internet for gathering stuff. 

Daniel Robert Prieto

In the new millennium arrived what was called Web 2.0 – the social Web. Static websites turned more interactive, making the editing and contributing much easier for people to interact and collaborate with each other. With this in mind, content aggregation platforms such as news and social media (Facebook, X, Instagram, Youtube, Flickr, etc.), forums, blogs, or collaborative wikis (such as online encyclopedia Wikipedia) started to receive content from users and build virtual communities. With those new platforms, users could then connect with friends and family, and share texts, images, and videos as well as opinions on a vast range of topics. To follow up on this file authorization metaphor, the Web essentially became a *read-write* model, which in essence is closer to what Tim Berners-Lee's original vision was:

> A collaborative medium, a place where we [could] all meet and read and write. [^BernersLee]

[^BernersLee1]: CERN, The birth of the Web, *CERN* [online], [no date], [https://home.cern/science/computing/birth-web](https://home.cern/science/computing/birth-web) [consulted: 27 October 2023]

[^BernersLee]: **LAWSON**, Mark, Berners-Lee on the read/write web*, BBC News [online], 9 August 2005, [http://news.bbc.co.uk/2/hi/technology/4132752.stm](http://news.bbc.co.uk/2/hi/technology/4132752.stm) [consulted: 16 October 2023]

Web 2.0 also gave birth to new models of business with the rise of online advertising and e-shops, permitting companies to generate revenues out of their online presence, and unfortunately led to a more corporate use of the Web, subject to censorship, data manipulation, and centralization of ownership. Companies such as Google, Amazon, Facebook (Meta), Apple and Microsoft (commonly grouped under the acronym GAFAM), own a large portion of the web. In parallel to the rise of such businesses and social media platforms at the turn of the new millennium, improvements in Internet speed, wireless connection, and hardware miniaturization gave birth to new ways for users to interact with the Web: hand-held multitouch devices such as smartphones and tablets. It is incredible to witness that in December 2010 the market share of mobile devices was only 4.1% compared to 95.9% for desktops. At the writing time of this thesis, in September 2023, the mobile market share rose to 53%, more than the desktops. [^stats] This significant change in behavior can also be explained by the rise of dedicated mobile applications released by device manufacturers’ application stores such as Apple's App Store or Google's Play Store. 

[^stats]: *Desktop vs Mobile vs Tablet Market Share Worldwide, *Statcounter GlobalStats [online chart], September 2023, [https://gs.statcounter.com/platform-market-share/desktop-mobile-tablet/worldwide](https://gs.statcounter.com/platform-market-share/desktop-mobile-tablet/worldwide) [consulted: 16 October 2023] 


## After Web 1.0 & Web2.0, Web3?

With recent advances in Machine Learning Algorithms, and ever-growing quantities of data uploaded online, further advances in information filtering systems such as targeted advertising or especially recommender systems through deep learning algorithms changed our online navigation and access to filtered informations. Recommenders, in which suggestions supposedly most pertinent are made to a user – alone or collaboratively with group of users of similar online behaviors, pool other user’s data collection – based on past online browsing behaviors. Considering the overwhelming amount of content we face while browsing a platform, recommender algorithms can prove useful, yet can raise questions as to our online decision-making process: what music we listen to, what date we meet, what news we read, and what item we buy. 

In the recent years, it seems social media reaches a point of fatigue from its audience, which slowly but surely, starts to evacuate the big social media companies, infested with user data collection and targeted ad technologies. This surveillance capitalism of making profit out people's private information, exploiting the psychic weaknesses of users, with the intent of keeping them on its platforms for as long as they can, _doom-scrolling_, raises flags to users, leading to distrust and fear of behavioral manipulation through misinformation from recommended content. Moreover, with so many platforms, services and disparate information competing for one’s attention through information overload and context collapse, users also start to look for slower-paced and more conscious Web browsing. 

> the book in itself, it's curated, in an order, it is selected, it's around a topic. You take that for granted, you think that reality is like this. When you go to the internet, that curation doesn't exist. It's a bucket. 

Daniel Robert Prieto

This also encourages a shift towards a more post-capital approach, towards the idea of collaborative commons, where users move from a system of scarcity to one of global abundance in sharing resources. This fostered an emerging typology of more independent social media platforms, between paid communities and/or decentralized alternatives. Meanwhile a comeback to vernacular uses of the web through digital gardening is interesting to study based on this current state of the Web. 

Following on the current evolutional aspects of the Web, a new term appears: _Web3_ or Web 3.0. This term designates the idea of a decentralized web based on blockchain technology, thus claiming to be the successor to Web 2.0, with several philosophical objectives: compromising censorship, giving Internet users back control over their data, combating the power of large platforms, reshuffling the cards of ownership on the Internet. Existing technologies are already shaping what will make up this third iteration of the Web: blockchain, artificial intelligence, cloud and edge computing, virtual reality, the Internet of Things (IoT), decentralized apps... Although we noticed Web 2.0 is criticized for its centralization of user data and oligopoly of platforms, the embryonic Web3 as it exists in 2022 is not immune to the centralization and consolidation of these few players into an oligopoly.



* Refs:
    * HAS THE INTERNET BECOME A HUMAN OR A THING, OR HAVE WE BECOME THE INTERNET? Z. BLAS, J. BRIDLE, C. DULLAART, O. KHOLEIF
    * [https://thewebisfucked.com/](https://thewebisfucked.com/) lol
    * J.R. Carpenter - A Handmade Web, \
[ http://luckysoap.com/statements/handmadeweb.html](http://luckysoap.com/statements/handmadeweb.html)
    * Olia Lialina - Vernacular Web, \
[ http://art.teleportacia.org/observation/vernacular/](http://art.teleportacia.org/observation/vernacular/)
    * Toby Shorin, Come for the Network, Pay for the Tool \
[ https://subpixel.space/entries/come-for-the-network-pay-for-the-tool/](https://subpixel.space/entries/come-for-the-network-pay-for-the-tool/)
    * Schwulst, Laurel, My website is a shifting house next to a river of knowledge. What could yours be?, 2018
    * https://otherinter.net/[ \
](https://subpixel.space/entries/come-for-the-network-pay-for-the-tool/)
* AI and the future of the web? 

(figure: content/images/Paul-Ford-Laurel-Schwulst.png caption: Quote from Paul Ford, 2016, Reboot the World, illustrated in Sparrows talking about the future of the web by Laurel Schwulst for The Library of Practical and Conceptual Resources)


## Conclusion part 1

Next, we will further introduce digital gathering methods and more specifically how user-generated content platforms turned user-generated-creative-curated-collaborative such as Are.na as main case study (among other platforms). 